! Code generated by ChatGPT-4, with minor edits such as declaring `n` integer
! This program solves the 2D Laplace equation using MPI and Fortran

program laplace
    use mpi

    implicit none

    integer :: ierr, num_procs, my_id
    integer :: i, j, iter, max_iter, n
    integer, dimension(2) :: neighbors
    integer, dimension(4) :: request
    integer, dimension(MPI_STATUS_SIZE) :: status
    double precision, dimension(:,:), allocatable :: u, u_old
    double precision :: h, error, tol

    ! Initialize MPI
    call MPI_INIT(ierr)
    call MPI_COMM_RANK(MPI_COMM_WORLD, my_id, ierr)
    call MPI_COMM_SIZE(MPI_COMM_WORLD, num_procs, ierr)

    ! Initialize problem parameters (these could be read from an input file)
    n = 100
    h = 0.1
    max_iter = 1000
    tol = 1e-6
    allocate(u(n,n), u_old(n,n))

    ! Initialize solution and old solution
    u = 0.0
    u_old = 0.0

    ! Set boundary conditions
    if (my_id == 0) u(:,1) = 1.0  ! Bottom boundary
    if (my_id == num_procs - 1) u(:,n) = 1.0  ! Top boundary
    u(1,:) = 1.0  ! Left boundary
    u(n,:) = 1.0  ! Right boundary

    ! Main loop
    do iter = 1, max_iter
        u_old = u

        ! Update solution
        do j = 2, n-1
            do i = 2, n-1
                u(i,j) = 0.25 * (u_old(i-1,j) + u_old(i+1,j) + u_old(i,j-1) + u_old(i,j+1))
            end do
        end do

        ! Exchange boundary data with neighbors
        call exchange_data(u, n, neighbors)

        ! Compute error
        error = maxval(abs(u - u_old))

        ! Check for convergence
        if (error < tol) exit
    end do

    print*, 'Finished after', iter, 'iterations with error', error

    ! Finalize MPI
    call MPI_FINALIZE(ierr)
end program laplace

subroutine exchange_data(u, n, neighbors)
    use mpi

    implicit none

    double precision, dimension(:,:), intent(inout) :: u
    integer, dimension(:), intent(in) :: neighbors
    integer, intent(in) :: n
    integer :: ierr
    integer, dimension(4) :: request
    integer, dimension(MPI_STATUS_SIZE) :: status

    ! Use non-blocking sends and receives for efficient communication
    ! Assuming neighbors(1) is the bottom neighbor and neighbors(2) is the top neighbor

    ! Send to bottom neighbor
    if (neighbors(1) >= 0) then
        call MPI_ISEND(u(:,2), n, MPI_DOUBLE_PRECISION, neighbors(1), 0, MPI_COMM_WORLD, request(1), ierr)
    end if

    ! Send to top neighbor
    if (neighbors(2) >= 0) then
        call MPI_ISEND(u(:,n-1), n, MPI_DOUBLE_PRECISION, neighbors(2), 0, MPI_COMM_WORLD, request(2), ierr)
    end if

    ! Receive from bottom neighbor
    if (neighbors(1) >= 0) then
        call MPI_IRECV(u(:,1), n, MPI_DOUBLE_PRECISION, neighbors(1), 0, MPI_COMM_WORLD, request(3), ierr)
    end if

    ! Receive from top neighbor
    if (neighbors(2) >= 0) then
        call MPI_IRECV(u(:,n), n, MPI_DOUBLE_PRECISION, neighbors(2), 0, MPI_COMM_WORLD, request(4), ierr)
    end if

    ! Wait for all communication to finish
    call MPI_WAITALL(4, request, status, ierr)

end subroutine exchange_data
